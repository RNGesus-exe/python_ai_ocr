{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8k-eKn4FinLA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import EMNIST\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "import cv2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0p7IOsLYK_6",
        "outputId": "2972ce67-cd9e-4c29-c1f5-20150d09c364"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zYguTottNrHt"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=1.0),  # Flip the image horizontally with 100% probability\n",
        "    transforms.RandomRotation((90, 90)),  # Rotate 90 degrees anti-clockwise\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Cy9T2g_-OMI6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to data/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/EMNIST/raw/gzip.zip to data/EMNIST/raw\n"
          ]
        }
      ],
      "source": [
        "train_dataset = EMNIST(root='data/', split='byclass', train=True, download=True, transform=transform)\n",
        "test_dataset = EMNIST(root='data/', split='byclass', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EbEv9YWqPzA9"
      },
      "outputs": [],
      "source": [
        "classes = train_dataset.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MQxlTXjAlMlb"
      },
      "outputs": [],
      "source": [
        "def show_image(img, label, classes):\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f'Label: {classes[label]}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z9TyVVOBoDeq",
        "outputId": "cd83b3f9-9962-409a-e31c-0273b2f1d90c"
      },
      "outputs": [],
      "source": [
        "#Only Run This if dataset is not in tensors\n",
        "\n",
        "# for i in range(5):\n",
        "    # image, label = train_dataset[i]\n",
        "    # show_image(image, label, classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pKPUxsdR1hpl"
      },
      "outputs": [],
      "source": [
        "# Create DataLoader for training dataset\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,  # Adjust batch size as needed\n",
        "    shuffle=True,   # Shuffle the training data\n",
        "    num_workers=2   # Number of subprocesses to use for data loading\n",
        ")\n",
        "\n",
        "# Create DataLoader for test dataset\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,  # Adjust batch size as needed\n",
        "    shuffle=False,  # No need to shuffle the test data\n",
        "    num_workers=2   # Number of subprocesses to use for data loading\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bNut2iKjYZFi"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN architecture\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 62)  # 62 classes in the 'byclass' split\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-PCGZcv6ZT0U"
      },
      "outputs": [],
      "source": [
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BXvmdDWWZYCw"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:  # Print every 100 mini-batches\n",
        "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GAD9nmnDZaN7"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "xgc7FuV8ZcSl",
        "outputId": "63eedc45-c886-4da6-b99f-1f3fd94156aa"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "# train_model(model, train_loader, criterion, optimizer, device, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('model_weights.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "6Y26Ju32ZdwJ",
        "outputId": "9805444a-e586-4063-cfb7-c8123170a269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 86.45%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "slri7ORsC9gd",
        "outputId": "852a0f91-d1d1-4b7c-ee7c-789ef2f09567"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    # Open the image file\n",
        "    img = Image.open(image_path).convert('L')  # Convert to grayscale ('L' mode)\n",
        "\n",
        "    # Resize the image to 28x28\n",
        "    img = img.resize((28, 28))\n",
        "\n",
        "    # Display the resized image\n",
        "    plt.figure()\n",
        "    plt.title(\"Resized Image (28x28)\")\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Invert the colors\n",
        "    img = ImageOps.invert(img)\n",
        "\n",
        "    _, img = cv2.threshold(np.array(img), 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Display the original grayscale image\n",
        "    plt.figure()\n",
        "    plt.title(\"Inverted Grayscale Image\")\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Define the transformations: convert to tensor and normalize\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    # Apply the transformations\n",
        "    img_tensor = transform(img)\n",
        "\n",
        "    # Add batch dimension (1, 1, 28, 28)\n",
        "    img_tensor = img_tensor.unsqueeze(0)\n",
        "\n",
        "    return img_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qpSewnOKDWVI"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "error",
          "evalue": "OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'threshold'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mletter_seg\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     13\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mletter_seg/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m file  \u001b[38;5;66;03m# Replace with your image path\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     processed_image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Process the image\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Predict the label of the image\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     predicted_label \u001b[38;5;241m=\u001b[39m predict_image(model, processed_image\u001b[38;5;241m.\u001b[39mto(device))\n",
            "Cell \u001b[0;32mIn[27], line 19\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Invert the colors\u001b[39;00m\n\u001b[1;32m     17\u001b[0m img \u001b[38;5;241m=\u001b[39m ImageOps\u001b[38;5;241m.\u001b[39minvert(img)\n\u001b[0;32m---> 19\u001b[0m _, img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m127\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTHRESH_BINARY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Display the original grayscale image\u001b[39;00m\n\u001b[1;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'threshold'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
          ]
        }
      ],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Function to predict the label of the processed image\n",
        "def predict_image(model, image_tensor):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        return predicted.item()\n",
        "\n",
        "# Example usage\n",
        "for file in os.listdir('letter_seg'):\n",
        "    image_path = 'letter_seg/' + file  # Replace with your image path\n",
        "    processed_image = preprocess_image(image_path)  # Process the image\n",
        "    \n",
        "\n",
        "    # Predict the label of the image\n",
        "    predicted_label = predict_image(model, processed_image.to(device))\n",
        "    print(f'Predicted Label: {predicted_label} | {classes[predicted_label]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

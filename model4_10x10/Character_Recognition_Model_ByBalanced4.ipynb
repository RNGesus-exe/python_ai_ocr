{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rhaNKy9cTgB5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import EMNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UeOMDV6-TjG0"
      },
      "outputs": [],
      "source": [
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=1.0),  # Flip the image horizontally with 100% probability\n",
        "    transforms.RandomRotation((90, 90)),  # Rotate 90 degrees anti-clockwise\n",
        "    transforms.Resize((10, 10)),  # Resize to 10x10\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7txmAarqTm4r",
        "outputId": "3b3bbc6c-84a9-47fd-a093-46b9bb43b793"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "train_dataset = EMNIST(root='data/', split='balanced', train=True, download=True, transform=transform)\n",
        "test_dataset = EMNIST(root='data/', split='balanced', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xGW8FRxzTpti"
      },
      "outputs": [],
      "source": [
        "# Split the training dataset into training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L0kyQdkaTriw"
      },
      "outputs": [],
      "source": [
        "# Define data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "classes = train_dataset.dataset.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XXAFeu6JTwe2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a simple CNN architecture for 10x10 images\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 2 * 2, 128)  # Adjusted for 10x10 input size\n",
        "        self.fc2 = nn.Linear(128, 47)  # 47 classes in the 'balanced' split\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 2 * 2)  # Adjusted for 10x10 input size\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "giNLEnExT1HU"
      },
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y6iryV8mT5gL"
      },
      "outputs": [],
      "source": [
        "# Function to train the model\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        running_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            running_total += labels.size(0)\n",
        "            running_corrects += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = 100 * running_corrects / running_total\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                images, labels = data\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                val_loss += criterion(outputs, labels).item() * labels.size(0)\n",
        "\n",
        "        val_accuracy = 100 * correct / total\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Train Acc: {train_accuracy:.3f}, Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}, Val Acc: {val_accuracy:.3f}')\n",
        "\n",
        "    print('Finished Training')\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pe3ZZ3XUEmA",
        "outputId": "a2887d6b-700f-4b13-c3f3-6fff185c7f3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "# train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10)\n",
        "model.load_state_dict(torch.load('model4_10x10.pth', weights_only=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ovEYw5GOUKGr"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "# torch.save(model.state_dict(), \"model4_10x10.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvw4dchgUVzq",
        "outputId": "88df8e92-c802-499e-d3b5-0a967811dc4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SimpleCNN(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=47, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to preprocess the image\n",
        "def preprocess_image(image_path, invert=False, resize=True, show_output=True):\n",
        "    img = Image.open(image_path).convert('L')\n",
        "\n",
        "    if invert:\n",
        "        img = ImageOps.invert(img)\n",
        "\n",
        "        if show_output:\n",
        "            plt.figure()\n",
        "            plt.title(\"Inverted Grayscale Image\")\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "    if resize:\n",
        "        img = img.resize((10, 10))\n",
        "\n",
        "        if show_output:\n",
        "            plt.figure()\n",
        "            plt.title(\"Resized Image (10x10)\")\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    img_tensor = transform(img)\n",
        "    img_tensor = img_tensor.unsqueeze(0)\n",
        "\n",
        "    return img, img_tensor\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "shvkFSDuUYR-"
      },
      "outputs": [],
      "source": [
        "def predict_image(model, image_tensor):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor.to(device))\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        return predicted.item(), probabilities.squeeze().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fdGo8gY3UaaG"
      },
      "outputs": [],
      "source": [
        "def create_output_dirs(output_dir):\n",
        "    if not os.path.isdir(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    for output_class in classes:\n",
        "        upper_class = output_class.upper()\n",
        "        dir_name = os.path.join(output_dir, upper_class)\n",
        "        if not os.path.isdir(dir_name):\n",
        "            os.makedirs(dir_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eEtKoVyhPEXi"
      },
      "outputs": [],
      "source": [
        "def get_image_output(image_path, output_dir, save_image=False, show_output=True):\n",
        "    create_output_dirs(output_dir)\n",
        "\n",
        "    processed_image, image_tensor = preprocess_image(image_path, show_output=show_output)\n",
        "\n",
        "    predicted_label, probabilities = predict_image(model, image_tensor)\n",
        "    upper_class_label = classes[predicted_label].upper()\n",
        "\n",
        "    if save_image:\n",
        "        output_name = os.path.basename(image_path)\n",
        "        processed_image.save(os.path.join(output_dir, upper_class_label, output_name))\n",
        "\n",
        "    if show_output:\n",
        "        print(f'Predicted Label: {classes[predicted_label]} (Uppercase: {upper_class_label})')\n",
        "\n",
        "        top5_probabilities_indices = np.argsort(probabilities)[-5:]\n",
        "        top5_probabilities_indices = top5_probabilities_indices[::-1]\n",
        "\n",
        "        print('Top 5 Probabilities:')\n",
        "        for idx in top5_probabilities_indices:\n",
        "            print(f'{classes[idx]}: {probabilities[idx]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "chPPd3d8V0ow"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'open1'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen1_10x10\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      4\u001b[0m     input_img \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder, img_filename)\n\u001b[1;32m      5\u001b[0m     get_image_output(image_path\u001b[38;5;241m=\u001b[39minput_img, output_dir\u001b[38;5;241m=\u001b[39moutput_folder, save_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'open1'"
          ]
        }
      ],
      "source": [
        "image_folder = \"open1\"\n",
        "output_folder = 'open1_10x10'\n",
        "for img_filename in os.listdir(image_folder):\n",
        "    input_img = os.path.join(image_folder, img_filename)\n",
        "    get_image_output(image_path=input_img, output_dir=output_folder, save_image=True, show_output=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zEKOMdJyW5D0",
        "outputId": "9717ab5c-7317-4ff1-8f3f-3d43870fcb75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/open1_10x10/ (stored 0%)\n",
            "  adding: content/open1_10x10/B/ (stored 0%)\n",
            "  adding: content/open1_10x10/B/open1.jpeg_96.png (deflated 6%)\n",
            "  adding: content/open1_10x10/B/open1.jpeg_113.png (deflated 6%)\n",
            "  adding: content/open1_10x10/B/open1.jpeg_188.png (deflated 1%)\n",
            "  adding: content/open1_10x10/B/open1.jpeg_85.png (deflated 6%)\n",
            "  adding: content/open1_10x10/B/open1.jpeg_90.png (deflated 9%)\n",
            "  adding: content/open1_10x10/Y/ (stored 0%)\n",
            "  adding: content/open1_10x10/Y/open1.jpeg_160.png (deflated 1%)\n",
            "  adding: content/open1_10x10/R/ (stored 0%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_28.png (deflated 7%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_120.png (deflated 7%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_74.png (stored 0%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_48.png (deflated 8%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_205.png (deflated 12%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_105.png (deflated 8%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_50.png (deflated 11%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_156.png (deflated 5%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_127.png (deflated 1%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_218.png (deflated 7%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_212.png (deflated 8%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_178.png (deflated 6%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_219.png (deflated 9%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_229.png (deflated 8%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_69.png (deflated 8%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_182.png (deflated 6%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_145.png (deflated 9%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_181.png (deflated 5%)\n",
            "  adding: content/open1_10x10/R/open1.jpeg_89.png (deflated 8%)\n",
            "  adding: content/open1_10x10/3/ (stored 0%)\n",
            "  adding: content/open1_10x10/G/ (stored 0%)\n",
            "  adding: content/open1_10x10/G/open1.jpeg_176.png (deflated 4%)\n",
            "  adding: content/open1_10x10/G/open1.jpeg_164.png (deflated 7%)\n",
            "  adding: content/open1_10x10/G/open1.jpeg_1.png (deflated 8%)\n",
            "  adding: content/open1_10x10/G/open1.jpeg_185.png (deflated 7%)\n",
            "  adding: content/open1_10x10/G/open1.jpeg_210.png (deflated 8%)\n",
            "  adding: content/open1_10x10/G/open1.jpeg_16.png (deflated 8%)\n",
            "  adding: content/open1_10x10/W/ (stored 0%)\n",
            "  adding: content/open1_10x10/W/open1.jpeg_86.png (deflated 3%)\n",
            "  adding: content/open1_10x10/W/open1.jpeg_56.png (deflated 3%)\n",
            "  adding: content/open1_10x10/W/open1.jpeg_100.png (stored 0%)\n",
            "  adding: content/open1_10x10/M/ (stored 0%)\n",
            "  adding: content/open1_10x10/M/open1.jpeg_7.png (deflated 2%)\n",
            "  adding: content/open1_10x10/M/open1.jpeg_34.png (deflated 3%)\n",
            "  adding: content/open1_10x10/M/open1.jpeg_68.png (deflated 3%)\n",
            "  adding: content/open1_10x10/M/open1.jpeg_125.png (deflated 1%)\n",
            "  adding: content/open1_10x10/O/ (stored 0%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_107.png (deflated 5%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_118.png (deflated 1%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_153.png (deflated 1%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_71.png (stored 0%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_217.png (deflated 1%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_177.png (deflated 1%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_167.png (deflated 1%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_123.png (deflated 1%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_25.png (deflated 8%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_4.png (deflated 7%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_57.png (deflated 7%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_201.png (stored 0%)\n",
            "  adding: content/open1_10x10/O/open1.jpeg_51.png (deflated 3%)\n",
            "  adding: content/open1_10x10/9/ (stored 0%)\n",
            "  adding: content/open1_10x10/H/ (stored 0%)\n",
            "  adding: content/open1_10x10/H/open1.jpeg_54.png (deflated 6%)\n",
            "  adding: content/open1_10x10/H/open1.jpeg_149.png (deflated 7%)\n",
            "  adding: content/open1_10x10/H/open1.jpeg_67.png (deflated 5%)\n",
            "  adding: content/open1_10x10/H/open1.jpeg_220.png (deflated 7%)\n",
            "  adding: content/open1_10x10/7/ (stored 0%)\n",
            "  adding: content/open1_10x10/Q/ (stored 0%)\n",
            "  adding: content/open1_10x10/E/ (stored 0%)\n",
            "  adding: content/open1_10x10/E/open1.jpeg_147.png (deflated 5%)\n",
            "  adding: content/open1_10x10/E/open1.jpeg_37.png (deflated 8%)\n",
            "  adding: content/open1_10x10/E/open1.jpeg_41.png (deflated 1%)\n",
            "  adding: content/open1_10x10/E/open1.jpeg_92.png (deflated 5%)\n",
            "  adding: content/open1_10x10/E/open1.jpeg_102.png (deflated 6%)\n",
            "  adding: content/open1_10x10/E/open1.jpeg_98.png (deflated 8%)\n",
            "  adding: content/open1_10x10/E/open1.jpeg_126.png (deflated 1%)\n",
            "  adding: content/open1_10x10/E/open1.jpeg_39.png (deflated 8%)\n",
            "  adding: content/open1_10x10/E/open1.jpeg_93.png (deflated 4%)\n",
            "  adding: content/open1_10x10/E/open1.jpeg_81.png (deflated 6%)\n",
            "  adding: content/open1_10x10/5/ (stored 0%)\n",
            "  adding: content/open1_10x10/5/open1.jpeg_47.png (stored 0%)\n",
            "  adding: content/open1_10x10/1/ (stored 0%)\n",
            "  adding: content/open1_10x10/1/open1.jpeg_192.png (deflated 8%)\n",
            "  adding: content/open1_10x10/1/open1.jpeg_103.png (deflated 7%)\n",
            "  adding: content/open1_10x10/1/open1.jpeg_58.png (deflated 7%)\n",
            "  adding: content/open1_10x10/1/open1.jpeg_83.png (deflated 11%)\n",
            "  adding: content/open1_10x10/D/ (stored 0%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_228.png (deflated 8%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_166.png (deflated 9%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_225.png (deflated 6%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_222.png (deflated 6%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_79.png (deflated 7%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_190.png (deflated 11%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_76.png (deflated 11%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_233.png (deflated 8%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_97.png (deflated 10%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_183.png (deflated 7%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_60.png (deflated 7%)\n",
            "  adding: content/open1_10x10/D/open1.jpeg_73.png (deflated 1%)\n",
            "  adding: content/open1_10x10/T/ (stored 0%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_138.png (deflated 7%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_55.png (deflated 6%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_23.png (deflated 1%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_194.png (deflated 8%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_38.png (deflated 8%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_221.png (deflated 8%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_21.png (deflated 10%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_235.png (deflated 7%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_53.png (deflated 5%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_134.png (deflated 9%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_196.png (deflated 8%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_135.png (deflated 7%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_109.png (deflated 10%)\n",
            "  adding: content/open1_10x10/T/open1.jpeg_116.png (deflated 8%)\n",
            "  adding: content/open1_10x10/F/ (stored 0%)\n",
            "  adding: content/open1_10x10/F/open1.jpeg_131.png (deflated 6%)\n",
            "  adding: content/open1_10x10/F/open1.jpeg_224.png (deflated 7%)\n",
            "  adding: content/open1_10x10/F/open1.jpeg_184.png (deflated 9%)\n",
            "  adding: content/open1_10x10/F/open1.jpeg_35.png (deflated 7%)\n",
            "  adding: content/open1_10x10/F/open1.jpeg_91.png (deflated 6%)\n",
            "  adding: content/open1_10x10/F/open1.jpeg_143.png (stored 0%)\n",
            "  adding: content/open1_10x10/F/open1.jpeg_195.png (deflated 8%)\n",
            "  adding: content/open1_10x10/F/open1.jpeg_231.png (deflated 8%)\n",
            "  adding: content/open1_10x10/F/open1.jpeg_117.png (deflated 7%)\n",
            "  adding: content/open1_10x10/F/open1.jpeg_121.png (deflated 5%)\n",
            "  adding: content/open1_10x10/F/open1.jpeg_191.png (deflated 8%)\n",
            "  adding: content/open1_10x10/K/ (stored 0%)\n",
            "  adding: content/open1_10x10/K/open1.jpeg_72.png (deflated 5%)\n",
            "  adding: content/open1_10x10/K/open1.jpeg_189.png (deflated 6%)\n",
            "  adding: content/open1_10x10/K/open1.jpeg_61.png (deflated 7%)\n",
            "  adding: content/open1_10x10/0/ (stored 0%)\n",
            "  adding: content/open1_10x10/0/open1.jpeg_202.png (stored 0%)\n",
            "  adding: content/open1_10x10/0/open1.jpeg_199.png (stored 0%)\n",
            "  adding: content/open1_10x10/0/open1.jpeg_80.png (deflated 6%)\n",
            "  adding: content/open1_10x10/0/open1.jpeg_22.png (stored 0%)\n",
            "  adding: content/open1_10x10/0/open1.jpeg_216.png (deflated 3%)\n",
            "  adding: content/open1_10x10/0/open1.jpeg_171.png (deflated 7%)\n",
            "  adding: content/open1_10x10/0/open1.jpeg_175.png (deflated 1%)\n",
            "  adding: content/open1_10x10/0/open1.jpeg_31.png (deflated 4%)\n",
            "  adding: content/open1_10x10/S/ (stored 0%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_62.png (deflated 6%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_148.png (deflated 4%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_152.png (deflated 4%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_12.png (deflated 4%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_5.png (deflated 5%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_101.png (deflated 6%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_200.png (deflated 5%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_43.png (deflated 5%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_20.png (deflated 10%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_159.png (deflated 5%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_108.png (deflated 4%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_151.png (stored 0%)\n",
            "  adding: content/open1_10x10/S/open1.jpeg_40.png (deflated 4%)\n",
            "  adding: content/open1_10x10/4/ (stored 0%)\n",
            "  adding: content/open1_10x10/V/ (stored 0%)\n",
            "  adding: content/open1_10x10/V/open1.jpeg_2.png (deflated 9%)\n",
            "  adding: content/open1_10x10/V/open1.jpeg_94.png (deflated 5%)\n",
            "  adding: content/open1_10x10/V/open1.jpeg_142.png (deflated 5%)\n",
            "  adding: content/open1_10x10/V/open1.jpeg_44.png (deflated 6%)\n",
            "  adding: content/open1_10x10/U/ (stored 0%)\n",
            "  adding: content/open1_10x10/U/open1.jpeg_179.png (deflated 8%)\n",
            "  adding: content/open1_10x10/U/open1.jpeg_198.png (deflated 2%)\n",
            "  adding: content/open1_10x10/U/open1.jpeg_146.png (deflated 6%)\n",
            "  adding: content/open1_10x10/U/open1.jpeg_168.png (deflated 1%)\n",
            "  adding: content/open1_10x10/U/open1.jpeg_10.png (deflated 7%)\n",
            "  adding: content/open1_10x10/U/open1.jpeg_213.png (deflated 6%)\n",
            "  adding: content/open1_10x10/U/open1.jpeg_0.png (deflated 4%)\n",
            "  adding: content/open1_10x10/U/open1.jpeg_19.png (stored 0%)\n",
            "  adding: content/open1_10x10/C/ (stored 0%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_84.png (deflated 4%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_13.png (stored 0%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_59.png (deflated 6%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_203.png (deflated 7%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_155.png (stored 0%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_154.png (deflated 7%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_141.png (deflated 1%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_209.png (deflated 1%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_49.png (deflated 7%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_52.png (deflated 6%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_204.png (deflated 4%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_227.png (deflated 7%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_26.png (deflated 5%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_82.png (deflated 8%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_66.png (deflated 9%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_6.png (deflated 8%)\n",
            "  adding: content/open1_10x10/C/open1.jpeg_173.png (deflated 5%)\n",
            "  adding: content/open1_10x10/X/ (stored 0%)\n",
            "  adding: content/open1_10x10/I/ (stored 0%)\n",
            "  adding: content/open1_10x10/J/ (stored 0%)\n",
            "  adding: content/open1_10x10/J/open1.jpeg_150.png (deflated 6%)\n",
            "  adding: content/open1_10x10/6/ (stored 0%)\n",
            "  adding: content/open1_10x10/6/open1.jpeg_8.png (deflated 6%)\n",
            "  adding: content/open1_10x10/Z/ (stored 0%)\n",
            "  adding: content/open1_10x10/N/ (stored 0%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_9.png (deflated 1%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_11.png (deflated 4%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_170.png (deflated 1%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_87.png (deflated 1%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_180.png (deflated 4%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_17.png (deflated 1%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_162.png (deflated 1%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_30.png (deflated 1%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_186.png (deflated 7%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_197.png (deflated 1%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_215.png (stored 0%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_70.png (deflated 1%)\n",
            "  adding: content/open1_10x10/N/open1.jpeg_211.png (deflated 1%)\n",
            "  adding: content/open1_10x10/8/ (stored 0%)\n",
            "  adding: content/open1_10x10/8/open1.jpeg_3.png (deflated 5%)\n",
            "  adding: content/open1_10x10/P/ (stored 0%)\n",
            "  adding: content/open1_10x10/P/open1.jpeg_18.png (deflated 6%)\n",
            "  adding: content/open1_10x10/P/open1.jpeg_206.png (deflated 1%)\n",
            "  adding: content/open1_10x10/P/open1.jpeg_158.png (deflated 5%)\n",
            "  adding: content/open1_10x10/P/open1.jpeg_207.png (deflated 6%)\n",
            "  adding: content/open1_10x10/P/open1.jpeg_111.png (deflated 8%)\n",
            "  adding: content/open1_10x10/P/open1.jpeg_78.png (deflated 6%)\n",
            "  adding: content/open1_10x10/P/open1.jpeg_110.png (deflated 5%)\n",
            "  adding: content/open1_10x10/A/ (stored 0%)\n",
            "  adding: content/open1_10x10/A/open1.jpeg_95.png (stored 0%)\n",
            "  adding: content/open1_10x10/A/open1.jpeg_208.png (deflated 5%)\n",
            "  adding: content/open1_10x10/A/open1.jpeg_42.png (deflated 4%)\n",
            "  adding: content/open1_10x10/A/open1.jpeg_223.png (deflated 6%)\n",
            "  adding: content/open1_10x10/A/open1.jpeg_140.png (deflated 1%)\n",
            "  adding: content/open1_10x10/A/open1.jpeg_46.png (deflated 5%)\n",
            "  adding: content/open1_10x10/A/open1.jpeg_64.png (stored 0%)\n",
            "  adding: content/open1_10x10/A/open1.jpeg_112.png (stored 0%)\n",
            "  adding: content/open1_10x10/A/open1.jpeg_214.png (stored 0%)\n",
            "  adding: content/open1_10x10/A/open1.jpeg_174.png (deflated 6%)\n",
            "  adding: content/open1_10x10/L/ (stored 0%)\n",
            "  adding: content/open1_10x10/L/open1.jpeg_104.png (deflated 8%)\n",
            "  adding: content/open1_10x10/L/open1.jpeg_226.png (deflated 3%)\n",
            "  adding: content/open1_10x10/L/open1.jpeg_106.png (deflated 5%)\n",
            "  adding: content/open1_10x10/L/open1.jpeg_172.png (deflated 2%)\n",
            "  adding: content/open1_10x10/2/ (stored 0%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r open1_10x10.zip /content/open1_10x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "15QdIlIlW5EB",
        "outputId": "95a256c8-17f8-4d98-99d1-72a0188dd65d"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_c0e6d282-06d0-47cb-999e-354dbbd11b47\", \"open1_10x10.zip\", 74929)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"open1_10x10.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing Ground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dataset(folder, classes=None):\n",
        "    if classes is None:\n",
        "        classes = [dir for dir in os.listdir(folder) if os.path.isdir(os.path.join(folder, dir))]\n",
        "        \n",
        "    dataset = defaultdict(list)\n",
        "    for img_class in classes:\n",
        "        class_folder = os.path.join(folder, img_class)\n",
        "        for img_name in os.listdir(class_folder):\n",
        "            img_path = os.path.join(class_folder, img_name)\n",
        "            dataset[img_class].append(img_path)\n",
        "            \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_total_accuracy(dataset, model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for img_class, img_paths in dataset.items():\n",
        "        for img_path in img_paths:\n",
        "            img, img_tensor = preprocess_image(img_path, show_output=False)\n",
        "            predicted_label, _ = predict_image(model, img_tensor)\n",
        "            if img_class == classes[predicted_label]:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return correct / total "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mean_probability_per_class(dataset, model):\n",
        "    predicition_probabilities = defaultdict(list)\n",
        "    for img_class, img_paths in dataset.items():\n",
        "        for img_path in img_paths:\n",
        "            img, img_tensor = preprocess_image(img_path, show_output=False)\n",
        "            _, probabilities = predict_image(model, img_tensor)\n",
        "            prob_correct_class = probabilities[classes.index(img_class)]\n",
        "            predicition_probabilities[img_class].append(prob_correct_class)\n",
        "            \n",
        "    mean_probabilities = {}\n",
        "    for img_class, probabilities in predicition_probabilities.items():\n",
        "        mean_probabilities[img_class] = np.mean(probabilities)\n",
        "    \n",
        "    return mean_probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For each class, get the true positive count, false positive count, and false negative count\n",
        "def get_image_statistics(parent_folder, images_names, model, classes):\n",
        "    resultant_stats = {}\n",
        "\n",
        "    for image_name in images_names:\n",
        "        data_stats = defaultdict(dict, {class_name: {'tp': 0, 'fp': 0, 'fn': 0} for class_name in classes})\n",
        "\n",
        "        dataset_folder = os.path.join(parent_folder, image_name, 'letters')\n",
        "        dataset = load_dataset(dataset_folder, classes)\n",
        "        \n",
        "        for true_class, img_paths in dataset.items():\n",
        "            for img_path in img_paths:\n",
        "                _, img_tensor = preprocess_image(img_path, show_output=False)\n",
        "                predicted_label, _ = predict_image(model, img_tensor)\n",
        "                predicted_class = classes[predicted_label]\n",
        "                \n",
        "                if predicted_class == true_class:\n",
        "                    data_stats[true_class]['tp'] += 1\n",
        "                else:\n",
        "                    data_stats[true_class]['fn'] += 1\n",
        "                    data_stats[predicted_class]['fp'] += 1\n",
        "        resultant_stats[image_name] = data_stats\n",
        "    \n",
        "    return resultant_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'get_accuracy' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m dataset_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parent_folder, image_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mletters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(dataset_folder, classes)\n\u001b[0;32m---> 12\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mget_accuracy\u001b[49m(dataset, model) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     13\u001b[0m accuracy_data[image_name] \u001b[38;5;241m=\u001b[39m accuracy\n\u001b[1;32m     15\u001b[0m mean_prob \u001b[38;5;241m=\u001b[39m get_mean_probability_per_class(dataset, model)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_accuracy' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "parent_folder = '/home/yilliee/ArhamSoft/Contour_NN_test'\n",
        "images_folder = os.path.join(parent_folder, 'test_images')\n",
        "images_names = os.listdir(images_folder)\n",
        "\n",
        "accuracy_data = {}\n",
        "probabilities_data = {}\n",
        "\n",
        "for image_name in images_names:\n",
        "    dataset_folder = os.path.join(parent_folder, image_name, 'letters')\n",
        "    dataset = load_dataset(dataset_folder, classes)\n",
        "    \n",
        "    accuracy = get_total_accuracy(dataset, model) * 100\n",
        "    accuracy_data[image_name] = accuracy\n",
        "        \n",
        "    mean_prob = get_mean_probability_per_class(dataset, model)\n",
        "    probabilities_data[image_name] = mean_prob\n",
        "    \n",
        "    print(f'For {image_name}: Accuracy : {accuracy:.4f} | Mean Probability of correct_class : {np.mean(list(mean_prob.values())):.4f}')\n",
        "\n",
        "mean_accuracy = np.mean(list(accuracy_data.values()))\n",
        "print(f'\\nMean accuracy: {mean_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

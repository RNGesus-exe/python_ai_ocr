{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8k-eKn4FinLA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import EMNIST\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0p7IOsLYK_6",
        "outputId": "47388229-cb0b-422d-8e79-78e755ee96c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zYguTottNrHt"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=1.0),  # Flip the image horizontally with 100% probability\n",
        "    transforms.RandomRotation((90, 90)),  # Rotate 90 degrees anti-clockwise\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Cy9T2g_-OMI6"
      },
      "outputs": [],
      "source": [
        "train_dataset = EMNIST(root='data/', split='byclass', train=True, download=True, transform=transform)\n",
        "test_dataset = EMNIST(root='data/', split='byclass', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbEv9YWqPzA9",
        "outputId": "381a5222-c663-4237-85cd-534ae192c463"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classes = train_dataset.classes\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MQxlTXjAlMlb",
        "outputId": "83d1142d-2423-4412-d282-f947ed391bc6"
      },
      "outputs": [],
      "source": [
        "# Function to display an image\n",
        "def imshow(img, label):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.title(f'Label: {label}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# # Select 3 random images from the dataset\n",
        "# random_indices = random.sample(range(len(train_dataset)), 5)\n",
        "\n",
        "# for idx in random_indices:\n",
        "#     img, label = train_dataset[idx]\n",
        "#     imshow(img, classes[label])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RMOhHeuwyabR"
      },
      "outputs": [],
      "source": [
        "validation_split = 0.1\n",
        "shuffle_dataset=True\n",
        "random_seed=42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3WrkPFayg8F",
        "outputId": "fb33a860-2c28-4f32-b879-5d6c426a34a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset Size: 697932\n"
          ]
        }
      ],
      "source": [
        "dataset_size = len(train_dataset)\n",
        "print(f\"Train Dataset Size: {dataset_size}\")\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset:\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=64, sampler=train_sampler, num_workers=1\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=64, sampler=val_sampler, num_workers=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pKPUxsdR1hpl"
      },
      "outputs": [],
      "source": [
        "# Create DataLoader for test dataset\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,  # Adjust batch size as needed\n",
        "    shuffle=False,  # No need to shuffle the test data\n",
        "    num_workers=1   # Number of subprocesses to use for data loading\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bNut2iKjYZFi"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN architecture\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 62)  # 62 classes in the 'byclass' split\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-PCGZcv6ZT0U"
      },
      "outputs": [],
      "source": [
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Learning rate {0.001 -> 86%}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XO8CTyTFFXyp"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    # Train the model\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        running_total = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            running_total += labels.size(0)\n",
        "            running_corrects += (predicted == labels).sum().item()\n",
        "\n",
        "            # if i % 100 == 99:\n",
        "                # print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}, Correct: {running_corrects / 100:.3f}, Total: {running_total / 100:.3f}')\n",
        "\n",
        "        # Calculate the training loss and training accuracy\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = 100 * running_corrects / running_total\n",
        "        train_losses.append(train_loss)  # Store train_loss\n",
        "\n",
        "        # evaluate on the validation set\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        val_loss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                images, labels = data\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                val_loss += criterion(outputs, labels).item() * labels.size(0)\n",
        "\n",
        "        # Calculate the validation accuracy and validation loss\n",
        "        val_accuracy = 100 * correct / total\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)  # Store val_loss\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Train Acc:{train_accuracy:.3f}, Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}, Val Acc: {val_accuracy:.3f}')\n",
        "\n",
        "    print('Finished Training')\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GAD9nmnDZaN7"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "xgc7FuV8ZcSl",
        "outputId": "10f6d960-b668-47cc-b7c7-0437af224f50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SimpleCNN(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=62, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "# train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10)\n",
        "# Run code from here to use pretrained weights\n",
        "model.load_state_dict(torch.load(\"model2_weights.pth\"))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "CbMXXDCxzbaZ",
        "outputId": "385d46d9-ac4b-44f5-e805-a9868689a10b"
      },
      "outputs": [],
      "source": [
        "# def plot_losses(train_losses, val_losses):\n",
        "#     epochs = range(1, len(train_losses) + 1)\n",
        "    \n",
        "#     plt.figure(figsize=(12, 6))\n",
        "    \n",
        "#     # Plot training loss\n",
        "#     plt.plot(epochs, train_losses, 'bo-', label='Train Loss')\n",
        "    \n",
        "#     # Plot validation loss\n",
        "#     plt.plot(epochs, val_losses, 'ro-', label='Validation Loss')\n",
        "    \n",
        "#     # Add labels and title\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.title('Training and Validation Loss over Epochs')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "#     plt.show()\n",
        "\n",
        "# plot_losses(train_losses, val_losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y26Ju32ZdwJ",
        "outputId": "19561edf-3ef3-4a91-c2d5-3710ed642123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 86.33%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v6T_uT2svUQt",
        "outputId": "d6fa031f-f825-4768-ad33-722519fc7a78"
      },
      "outputs": [],
      "source": [
        "# def show_misclassified_images(model, dataloader, device):\n",
        "#     model.eval()\n",
        "#     misclassified = []\n",
        "#     with torch.no_grad():\n",
        "#         for images, labels in dataloader:\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = model(images)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             for i in range(len(labels)):\n",
        "#                 if predicted[i] != labels[i]:\n",
        "#                     misclassified.append((images[i].cpu(), predicted[i].item(), labels[i].item()))\n",
        "\n",
        "#     for img, pred, actual in misclassified[:10]:  # Show the first 10 misclassified images\n",
        "#         img = img.squeeze().numpy()\n",
        "#         plt.imshow(img, cmap='gray')\n",
        "#         plt.title(f'Predicted: {classes[pred]}, Actual: {classes[actual]}')\n",
        "#         plt.show()\n",
        "\n",
        "# # Analyze misclassifications\n",
        "# show_misclassified_images(model, val_loader, device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "slri7ORsC9gd",
        "outputId": "852a0f91-d1d1-4b7c-ee7c-789ef2f09567"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the image\n",
        "def preprocess_image(image_path, invert=False, resize=True, show_output=True):\n",
        "    # Open the image file\n",
        "    img = Image.open(image_path).convert('L')  # Convert to grayscale ('L' mode)\n",
        "\n",
        "    if invert:\n",
        "        # Invert the colors\n",
        "        img = ImageOps.invert(img)\n",
        "\n",
        "        if show_output:\n",
        "            # Display the original grayscale image\n",
        "            plt.figure()\n",
        "            plt.title(\"Inverted Grayscale Image\")\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "    if resize:\n",
        "        # Resize the image to 28x28\n",
        "        img = img.resize((28, 28))\n",
        "\n",
        "        if show_output:\n",
        "            # Display the resized image\n",
        "            plt.figure()\n",
        "            plt.title(\"Resized Image (28x28)\")\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "    # Define the transformations: convert to tensor and normalize\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    # Apply the transformations\n",
        "    img_tensor = transform(img)\n",
        "\n",
        "    # Add batch dimension (1, 1, 28, 28)\n",
        "    img_tensor = img_tensor.unsqueeze(0)\n",
        "\n",
        "    return img, img_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qpSewnOKDWVI"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Label: B\n",
            "Top 5 Probabilities:\n",
            "B: 0.9996\n",
            "C: 0.0003\n",
            "Q: 0.0001\n",
            "S: 0.0000\n",
            "G: 0.0000\n"
          ]
        }
      ],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Function to predict the label of the processed image\n",
        "def predict_image(model, image_tensor):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor.to(device))\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        return predicted.item(), probabilities.squeeze().cpu().numpy()\n",
        "\n",
        "# Example usage\n",
        "image_path = 'letter_seg/4.png'  # Replace with your image path\n",
        "processed_image = preprocess_image(image_path)  # Process the image\n",
        "\n",
        "predicted_label, probabilities = predict_image(model, processed_image)\n",
        "\n",
        "# Predict the label of the image\n",
        "print(f'Predicted Label: {classes[predicted_label]}')\n",
        "\n",
        "# Get the top 5 probabilities and their indices\n",
        "top5_probabilities_indices = np.argsort(probabilities)[-5:]  # Get indices of top 5 probabilities\n",
        "top5_probabilities_indices = top5_probabilities_indices[::-1]  # Reverse to get descending order\n",
        "\n",
        "# Print the probabilities for all classes\n",
        "print('Top 5 Probabilities:')\n",
        "for idx in top5_probabilities_indices:\n",
        "    print(f'{classes[idx]}: {probabilities[idx]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SimpleCNN(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=62, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run code from here to use pretrained weights\n",
        "model.load_state_dict(torch.load(\"model2_weights.pth\"))\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Label: D\n",
            "Top 5 Probabilities:\n",
            "D: 0.8661\n",
            "T: 0.1272\n",
            "S: 0.0061\n",
            "J: 0.0003\n",
            "F: 0.0002\n"
          ]
        }
      ],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Function to predict the label of the processed image\n",
        "def predict_image(model, image_tensor):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor.to(device))\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        return predicted.item(), probabilities.squeeze().cpu().numpy()\n",
        "\n",
        "# Example usage\n",
        "image_path = 'letter_seg/2.png'  # Replace with your image path\n",
        "processed_image = preprocess_image(image_path)  # Process the image\n",
        "\n",
        "predicted_label, probabilities = predict_image(model, processed_image)\n",
        "\n",
        "# Predict the label of the image\n",
        "print(f'Predicted Label: {classes[predicted_label]}')\n",
        "\n",
        "# Get the top 5 probabilities and their indices\n",
        "top5_probabilities_indices = np.argsort(probabilities)[-5:]  # Get indices of top 5 probabilities\n",
        "top5_probabilities_indices = top5_probabilities_indices[::-1]  # Reverse to get descending order\n",
        "\n",
        "# Print the probabilities for all classes\n",
        "print('Top 5 Probabilities:')\n",
        "for idx in top5_probabilities_indices:\n",
        "    print(f'{classes[idx]}: {probabilities[idx]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing ground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dataset(folder, classes=None):\n",
        "    if classes is None:\n",
        "        classes = [dir for dir in os.listdir(folder) if os.path.isdir(os.path.join(folder, dir))]\n",
        "        \n",
        "    dataset = defaultdict(list)\n",
        "    for img_class in classes:\n",
        "        class_folder = os.path.join(folder, img_class)\n",
        "        if not os.path.isdir(class_folder):\n",
        "            continue\n",
        "        for img_name in os.listdir(class_folder):\n",
        "            img_path = os.path.join(class_folder, img_name)\n",
        "            dataset[img_class].append(img_path)\n",
        "            \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_total_accuracy(dataset, model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for img_class, img_paths in dataset.items():\n",
        "        for img_path in img_paths:\n",
        "            img, img_tensor = preprocess_image(img_path, show_output=False)\n",
        "            predicted_label, _ = predict_image(model, img_tensor)\n",
        "            if img_class == classes[predicted_label]:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mean_probability_per_class(dataset, model):\n",
        "    predicition_probabilities = defaultdict(list)\n",
        "    for img_class, img_paths in dataset.items():\n",
        "        for img_path in img_paths:\n",
        "            img, img_tensor = preprocess_image(img_path, show_output=False)\n",
        "            _, probabilities = predict_image(model, img_tensor)\n",
        "            prob_correct_class = probabilities[classes.index(img_class)]\n",
        "            predicition_probabilities[img_class].append(prob_correct_class)\n",
        "            \n",
        "    mean_probabilities = {}\n",
        "    for img_class, probabilities in predicition_probabilities.items():\n",
        "        mean_probabilities[img_class] = np.mean(probabilities)\n",
        "    \n",
        "    return mean_probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For each class, get the true positive count, false positive count, and false negative count\n",
        "def get_image_statistics(parent_folder, images_names, model, classes):\n",
        "    resultant_stats = {}\n",
        "\n",
        "    for image_name in images_names:\n",
        "        data_stats = defaultdict(dict, {class_name: {'tp': 0, 'fp': 0, 'fn': 0} for class_name in classes})\n",
        "\n",
        "        dataset_folder = os.path.join(parent_folder, image_name, 'letters')\n",
        "        dataset = load_dataset(dataset_folder, classes)\n",
        "        \n",
        "        for true_class, img_paths in dataset.items():\n",
        "            for img_path in img_paths:\n",
        "                _, img_tensor = preprocess_image(img_path, show_output=False)\n",
        "                predicted_label, _ = predict_image(model, img_tensor)\n",
        "                predicted_class = classes[predicted_label]\n",
        "                \n",
        "                if predicted_class == true_class:\n",
        "                    data_stats[true_class]['tp'] += 1\n",
        "                else:\n",
        "                    data_stats[true_class]['fn'] += 1\n",
        "                    data_stats[predicted_class]['fp'] += 1\n",
        "        resultant_stats[image_name] = data_stats\n",
        "    \n",
        "    return resultant_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For open2.jpeg: Accuracy : 66.2222 | Mean Probability of correct_class : 0.7461\n",
            "For sample1.jpeg: Accuracy : 59.0909 | Mean Probability of correct_class : 0.6806\n",
            "For blank.jpeg: Accuracy : 51.0000 | Mean Probability of correct_class : 0.6038\n",
            "For open1.jpeg: Accuracy : 65.9898 | Mean Probability of correct_class : 0.6555\n",
            "\n",
            "Mean accuracy: 60.5757\n"
          ]
        }
      ],
      "source": [
        "\n",
        "parent_folder = '/home/yilliee/ArhamSoft/Contour_NN_test'\n",
        "images_folder = os.path.join(parent_folder, 'test_images')\n",
        "images_names = os.listdir(images_folder)\n",
        "\n",
        "accuracy_data = {}\n",
        "probabilities_data = {}\n",
        "\n",
        "for image_name in images_names:\n",
        "    dataset_folder = os.path.join(parent_folder, image_name, 'letters')\n",
        "    dataset = load_dataset(dataset_folder, classes)\n",
        "    \n",
        "    accuracy = get_total_accuracy(dataset, model) * 100\n",
        "    accuracy_data[image_name] = accuracy\n",
        "        \n",
        "    mean_prob = get_mean_probability_per_class(dataset, model)\n",
        "    probabilities_data[image_name] = mean_prob\n",
        "    \n",
        "    print(f'For {image_name}: Accuracy : {accuracy:.4f} | Mean Probability of correct_class : {np.mean(list(mean_prob.values())):.4f}')\n",
        "\n",
        "mean_accuracy = np.mean(list(accuracy_data.values()))\n",
        "print(f'\\nMean accuracy: {mean_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "per_image_stats = get_image_statistics(parent_folder, images_names, model, classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pd.ExcelWriter('model2_stats.xlsx') as writer:\n",
        "    for image_name, image_stats in per_image_stats.items():\n",
        "        df = pd.DataFrame(per_image_stats[image_name]).T\n",
        "        df.to_excel(excel_writer=writer, sheet_name=image_name, index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing ground"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
